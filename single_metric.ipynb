{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Graph Analysis\n",
    "### *(Individual Metrics)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sampling for entire analysis\n",
    "data_astronomy1 = pd.read_excel('data/Astronomy.xlsx', sheet_name='강민철')\n",
    "data_astronomy2 = pd.read_excel('data/Astronomy.xlsx', sheet_name='강지헌')\n",
    "\n",
    "# Drop an useless column\n",
    "data_astronomy1.drop('Name', axis=1, inplace=True)\n",
    "data_astronomy2.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# Merge all dataframes into one dataframe:\n",
    "data_astronomy = pd.concat([data_astronomy1, data_astronomy2], axis=0)\n",
    "data_astronomy.drop(data_astronomy.loc[data_astronomy['ID'] == 53].index[0], inplace=True)\n",
    "# data_astronomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling1 = pd.read_excel('data/Sampling.xlsx', sheet_name='강지헌')\n",
    "data_sampling2 = pd.read_excel('data/Sampling.xlsx', sheet_name='신아현')\n",
    "data_sampling3 = pd.read_excel('data/Sampling.xlsx', sheet_name='신수연')\n",
    "\n",
    "data_sampling1.drop('Name', axis=1, inplace=True)\n",
    "data_sampling2.drop('Name', axis=1, inplace=True)\n",
    "data_sampling3.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "data_sampling = pd.concat([data_sampling1, data_sampling2, data_sampling3], axis=0)\n",
    "# data_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_database1 = pd.read_excel('data/Database.xlsx', sheet_name='신수연')\n",
    "data_database2 = pd.read_excel('data/Database.xlsx', sheet_name='양연선')\n",
    "data_database3 = pd.read_excel('data/Database.xlsx', sheet_name='김나영')\n",
    "\n",
    "data_database1.drop('Name', axis=1, inplace=True)\n",
    "data_database2.drop('Name', axis=1, inplace=True)\n",
    "data_database3.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "data_database = pd.concat([data_database1, data_database2, data_database3], axis=0)\n",
    "# data_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process graphs\n",
    "- 이름 형식: `<Domain>_<Modality>_<ID>`\n",
    "- Domain: `ASTRONOMY`, `SAMPLING`, `DATABASE`\n",
    "\n",
    "***참고사항**:\n",
    "    혹시 몰라 노드에 P.Knowledge를 추가로 라벨링해두었지만, 분석할 때는 그냥 df상에서도 충분히 가능해보임*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_astronomy = {}\n",
    "\n",
    "for id, sub_df in data_astronomy.groupby('ID'):\n",
    "    # New graph object\n",
    "    graph_name = f\"Astronomy_{sub_df['Mod.'].iloc[0]}_{sub_df['ID'].iloc[0]}\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for _, row in sub_df.iterrows():\n",
    "        start_node = row['Start']\n",
    "        if pd.notna(row['End']):\n",
    "            end_nodes = [end_node.rstrip() for end_node in row['End'].split(',')]\n",
    "            for end_node in end_nodes:\n",
    "                G.add_edge(start_node, end_node)\n",
    "        # Add p.knowledge labels:  O -> 1(true)  |  X -> 0(false)\n",
    "        try:\n",
    "            G.nodes[start_node]['P.Knowledge'] = 1 if row['P.Knowledge'] == 'O' else 0\n",
    "        except KeyError:\n",
    "            G.add_node(start_node)\n",
    "            G.nodes[start_node]['P.Knowledge'] = 0\n",
    "    \n",
    "    # Save the graph\n",
    "    graphs_astronomy[graph_name] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_sampling = {}\n",
    "\n",
    "for id, sub_df in data_sampling.groupby('ID'):\n",
    "    # New graph object\n",
    "    graph_name = f\"Sampling_{sub_df['Mod.'].iloc[0]}_{sub_df['ID'].iloc[0]}\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for _, row in sub_df.iterrows():\n",
    "        start_node = row['Start']\n",
    "        if pd.notna(row['End']):\n",
    "            end_nodes = [end_node.rstrip() for end_node in row['End'].split(',')]\n",
    "            for end_node in end_nodes:\n",
    "                G.add_edge(start_node, end_node)\n",
    "        # Add p.knowledge labels:  O -> 1(true)  |  X -> 0(false)\n",
    "        try:\n",
    "            G.nodes[start_node]['P.Knowledge'] = 1 if row['P.Knowledge'] == 'O' else 0\n",
    "        except KeyError:\n",
    "            G.add_node(start_node)\n",
    "            G.nodes[start_node]['P.Knowledge'] = 0\n",
    "    \n",
    "    # Save the graph\n",
    "    graphs_sampling[graph_name] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_database = {}\n",
    "\n",
    "for id, sub_df in data_database.groupby('ID'):\n",
    "    # New graph object\n",
    "    graph_name = f\"Database_{sub_df['Mod.'].iloc[0]}_{sub_df['ID'].iloc[0]}\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges\n",
    "    for _, row in sub_df.iterrows():\n",
    "        start_node = row['Start']\n",
    "        if pd.notna(row['End']):\n",
    "            end_nodes = [end_node.rstrip() for end_node in row['End'].split(',')]\n",
    "            for end_node in end_nodes:\n",
    "                G.add_edge(start_node, end_node)\n",
    "        # Add p.knowledge labels:  O -> 1(true)  |  X -> 0(false)\n",
    "        try:\n",
    "            G.nodes[start_node]['P.Knowledge'] = 1 if row['P.Knowledge'] == 'O' else 0\n",
    "        except KeyError:\n",
    "            G.add_node(start_node)\n",
    "            G.nodes[start_node]['P.Knowledge'] = 0\n",
    "    \n",
    "    # Save the graph\n",
    "    graphs_database[graph_name] = G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of the dataframes into dictionaries\n",
    "def get_root_dict(df: pd.DataFrame) -> dict:\n",
    "    result_dict = {}\n",
    "    for id, sub_df in df.groupby('ID'):\n",
    "        result_dict[str(id)] = sub_df[sub_df['Root'] == 'O']['Start'].tolist()\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots_astronomy = get_root_dict(data_astronomy)\n",
    "roots_sampling = get_root_dict(data_sampling)\n",
    "roots_database = get_root_dict(data_database)\n",
    "\n",
    "# If you want to see the dictionaries, you can uncomment the following lines\n",
    "# print(roots_astronomy)\n",
    "# print(roots_sampling)\n",
    "# print(roots_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_id(text):\n",
    "    pattern = r'\\d+$'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for individual metrics\n",
    "1. `num_of_concepts`:\n",
    "   - 노드 개수와 동일\n",
    "2. `num_of_relationships`\n",
    "   - 엣지 개수와 동일\n",
    "3. `num_of_hierarchies`:\n",
    "   - 제대로 이해한 것이 맞다면, deepest hierarchy와 1 차이나는 별 의미 없는 값\n",
    "4. `num_of_crosslinks`:\n",
    "   - MST를 생성하여 MST에 포함되지 않은 엣지 개수를 카운트\n",
    "5. `cal_aspl`:\n",
    "   - 가중평균 값 계산\n",
    "6. `cal_cc`:\n",
    "   - 가중평균 값 계산\n",
    "   - 삼각형 구조가 없으면 0이 될 수 있음\n",
    "7. `cal_network_density`\n",
    "8. `cal_complexity`\n",
    "9.  `get_level_deepest_hierarchy`:\n",
    "    - Root로부터 가장 긴 체인의 길이 (Root 포함)\n",
    "    - 2개 이상의 root 존재 시, max 값 선택\n",
    "10. `num_of_components`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_concepts(G):\n",
    "    return G.number_of_nodes()\n",
    "\n",
    "def num_of_relationships(G):\n",
    "    return G.number_of_edges()\n",
    "\n",
    "# def num_of_hierarchies(G):\n",
    "#     return nx.number_of_strongly_connected_components(G)\n",
    "\n",
    "def num_of_crosslinks(G):\n",
    "    crosslink_count = 0\n",
    "    \n",
    "    # Handle connected components separately\n",
    "    if G.is_directed():\n",
    "        components = [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]\n",
    "    else:\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    \n",
    "    for component in components:\n",
    "        # Convert directed graph to undirected for minimum spanning tree calculation\n",
    "        undirected_component = component.to_undirected()\n",
    "        # Calculate the minimum spanning tree (MST) of the undirected component\n",
    "        mst = nx.minimum_spanning_tree(undirected_component)\n",
    "        \n",
    "        # Crosslinks are edges in the original component that are not in the MST\n",
    "        for u, v in component.edges():\n",
    "            if not mst.has_edge(u, v):\n",
    "                crosslink_count += 1\n",
    "    \n",
    "    return crosslink_count\n",
    "\n",
    "def cal_aspl(G):\n",
    "    # 강제 변환\n",
    "    if G.is_directed():\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    # Find components\n",
    "    components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    \n",
    "    total_aspl = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    for component in components:\n",
    "        n = len(component)\n",
    "        if n > 1:  # 최소 두 개의 노드가 있는 컴포넌트만 고려\n",
    "            try:\n",
    "                aspl = nx.average_shortest_path_length(component)\n",
    "                total_aspl += aspl * n\n",
    "                total_nodes += n\n",
    "            except nx.NetworkXError:\n",
    "                continue\n",
    "    \n",
    "    # 가중 평균\n",
    "    avg_aspl = total_aspl / total_nodes if total_nodes > 0 else 0\n",
    "\n",
    "    return avg_aspl\n",
    "\n",
    "def cal_cc(G):\n",
    "    # Find components\n",
    "    components = [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]\n",
    "    \n",
    "    # Initialize total sum and total nodes count\n",
    "    total_weighted_cc = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    # Calculate weighted CC for each component\n",
    "    for component in components:\n",
    "        num_nodes = component.number_of_nodes()\n",
    "        component_cc = nx.average_clustering(component)\n",
    "        total_weighted_cc += component_cc * num_nodes\n",
    "        total_nodes += num_nodes\n",
    "    \n",
    "    # Calculate weighted average CC\n",
    "    avg_cc = total_weighted_cc / total_nodes if total_nodes > 0 else 0\n",
    "    \n",
    "    return avg_cc\n",
    "\n",
    "def cal_network_density(G):\n",
    "    return nx.density(G)\n",
    "\n",
    "def cal_complexity(G):\n",
    "    E = G.number_of_edges()\n",
    "    N = G.number_of_nodes()\n",
    "    return (E / N) if (N > 0) else 0\n",
    "\n",
    "def get_level_deepest_hierarchy(G, roots_list):\n",
    "    max_hierarchy_level = 0\n",
    "    \n",
    "    # Handle connected components separately\n",
    "    if G.is_directed():\n",
    "        components = [G.subgraph(c).copy() for c in nx.weakly_connected_components(G)]\n",
    "    else:\n",
    "        components = [G.subgraph(c).copy() for c in nx.connected_components(G)]\n",
    "    \n",
    "    # Iterate through each component and find the maximum hierarchy level\n",
    "    for component in components:\n",
    "        for root in roots_list:\n",
    "            if root in component:  # root가 component에 있는지 확인\n",
    "                # Calculate the depth of the hierarchy from this root node\n",
    "                hierarchy_levels = nx.single_source_shortest_path_length(component, root).values()\n",
    "                max_hierarchy_level = max(max_hierarchy_level, max(hierarchy_levels))\n",
    "    \n",
    "    return max_hierarchy_level + 1  # Adding 1 to include the root level itself\n",
    "\n",
    "def num_of_components(G):\n",
    "    if G.is_directed():\n",
    "        return len(list(nx.weakly_connected_components(G)))\n",
    "    else:\n",
    "        return len(list(nx.connected_components(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis for individual metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new dataframe for the analysis\n",
    "columns = [\"ID\", \"num_of_concepts\", \"num_of_relationships\", \"num_of_crosslinks\", \"ASPL\", \"CC\", \"network_density\", \"complexity\", \"level_of_deepest_hierarchy\", \"num_of_components\"]\n",
    "indiv_astronomy = pd.DataFrame(columns=columns).set_index(\"ID\")\n",
    "indiv_sampling = pd.DataFrame(columns=columns).set_index(\"ID\")\n",
    "indiv_database = pd.DataFrame(columns=columns).set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_astronomy.items():\n",
    "    try:\n",
    "        indiv_astronomy.loc[graph_name] = [\n",
    "            num_of_concepts(G), \n",
    "            num_of_relationships(G), \n",
    "            num_of_crosslinks(G), \n",
    "            cal_aspl(G), \n",
    "            cal_cc(G), \n",
    "            cal_network_density(G), \n",
    "            cal_complexity(G), \n",
    "            get_level_deepest_hierarchy(G, roots_astronomy[extract_id(graph_name)]),\n",
    "            num_of_components(G)\n",
    "        ]\n",
    "    except nx.NetworkXNoPath as e:\n",
    "        print(f\"Path error at {graph_name}: {e}\")\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "indiv_astronomy.to_excel(\"result/Astronomy_indiv.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_sampling.items():\n",
    "    try:\n",
    "        indiv_sampling.loc[graph_name] = [\n",
    "            num_of_concepts(G), \n",
    "            num_of_relationships(G), \n",
    "            num_of_crosslinks(G), \n",
    "            cal_aspl(G), \n",
    "            cal_cc(G), \n",
    "            cal_network_density(G), \n",
    "            cal_complexity(G), \n",
    "            get_level_deepest_hierarchy(G, roots_sampling[extract_id(graph_name)]),\n",
    "            num_of_components(G)\n",
    "        ]\n",
    "    except nx.NetworkXNoPath as e:\n",
    "        print(f\"Path error at {graph_name}: {e}\")\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "indiv_sampling.to_excel(\"result/Sampling_indiv.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_database.items():\n",
    "    try:\n",
    "        indiv_database.loc[graph_name] = [\n",
    "            num_of_concepts(G), \n",
    "            num_of_relationships(G), \n",
    "            num_of_crosslinks(G), \n",
    "            cal_aspl(G), \n",
    "            cal_cc(G), \n",
    "            cal_network_density(G), \n",
    "            cal_complexity(G), \n",
    "            get_level_deepest_hierarchy(G, roots_database[extract_id(graph_name)]),\n",
    "            num_of_components(G)\n",
    "        ]\n",
    "    except nx.NetworkXNoPath as e:\n",
    "        print(f\"Path error at {graph_name}: {e}\")\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "indiv_database.to_excel(\"result/Database_indiv.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for centrality metrics\n",
    "\n",
    "1. `cal_node_betweeness`\n",
    "2. `cal_node_closeness`\n",
    "3. `cal_node_degree`\n",
    "\n",
    "*(각각 상위 3개의 항목을 추출함)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_node_betweenness(G):\n",
    "    values = nx.betweenness_centrality(G)\n",
    "    top_3_nodes = sorted(values.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "    return dict(top_3_nodes)\n",
    "\n",
    "def cal_node_closeness(G):\n",
    "    values = nx.closeness_centrality(G)\n",
    "    top_3_nodes = sorted(values.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "    return dict(top_3_nodes)\n",
    "\n",
    "def cal_node_degree(G):\n",
    "    values = nx.degree_centrality(G)\n",
    "    top_3_nodes = sorted(values.items(), key=lambda item: item[1], reverse=True)[:3]\n",
    "    return dict(top_3_nodes)\n",
    "\n",
    "def cal_betweenness_average(G):\n",
    "    G = nx.DiGraph(G)\n",
    "    values = nx.betweenness_centrality(G)\n",
    "    return np.mean(list(values.values()))\n",
    "\n",
    "def cal_closeness_average(G):\n",
    "    G = nx.DiGraph(G)\n",
    "    values = nx.closeness_centrality(G)\n",
    "    return np.mean(list(values.values()))\n",
    "\n",
    "def cal_degree_average(G):\n",
    "    G = nx.DiGraph(G)\n",
    "    values = nx.degree_centrality(G)\n",
    "    return np.mean(list(values.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis for centrality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_centrality_to_dataframe(df, graph_name, centrality_data, centrality_type, mean_value):\n",
    "    df.loc[graph_name, (centrality_type, 'node1')] = list(centrality_data.keys())[0]\n",
    "    df.loc[graph_name, (centrality_type, 'value1')] = list(centrality_data.values())[0]\n",
    "    df.loc[graph_name, (centrality_type, 'node2')] = list(centrality_data.keys())[1]\n",
    "    df.loc[graph_name, (centrality_type, 'value2')] = list(centrality_data.values())[1]\n",
    "    df.loc[graph_name, (centrality_type, 'node3')] = list(centrality_data.keys())[2]\n",
    "    df.loc[graph_name, (centrality_type, 'value3')] = list(centrality_data.values())[2]\n",
    "    \n",
    "    df.loc[graph_name, (centrality_type, 'mean')] = mean_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most important node in each graph\n",
    "columns = pd.MultiIndex.from_product([['Betweenness', 'Closeness', 'Degree'], ['node1', 'value1', 'node2', 'value2', 'node3', 'value3', 'mean']])\n",
    "central_astronomy = pd.DataFrame(columns=columns)\n",
    "central_sampling = pd.DataFrame(columns=columns)\n",
    "central_database = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_astronomy.items():\n",
    "    betweenness_top_3 = cal_node_betweenness(G)\n",
    "    closeness_top_3 = cal_node_closeness(G)\n",
    "    degree_top_3 = cal_node_degree(G)\n",
    "    \n",
    "    add_centrality_to_dataframe(central_astronomy, graph_name, betweenness_top_3, 'Betweenness', cal_betweenness_average(G))\n",
    "    add_centrality_to_dataframe(central_astronomy, graph_name, closeness_top_3, 'Closeness', cal_closeness_average(G))\n",
    "    add_centrality_to_dataframe(central_astronomy, graph_name, degree_top_3, 'Degree', cal_degree_average(G))\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "central_astronomy.to_excel(\"result/Astronomy_central.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_sampling.items():\n",
    "    betweenness_top_3 = cal_node_betweenness(G)\n",
    "    closeness_top_3 = cal_node_closeness(G)\n",
    "    degree_top_3 = cal_node_degree(G)\n",
    "    \n",
    "    add_centrality_to_dataframe(central_sampling, graph_name, betweenness_top_3, 'Betweenness', cal_betweenness_average(G))\n",
    "    add_centrality_to_dataframe(central_sampling, graph_name, closeness_top_3, 'Closeness', cal_closeness_average(G))\n",
    "    add_centrality_to_dataframe(central_sampling, graph_name, degree_top_3, 'Degree', cal_degree_average(G))\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "central_sampling.to_excel(\"result/Sampling_central.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph_name, G in graphs_database.items():\n",
    "    betweenness_top_3 = cal_node_betweenness(G)\n",
    "    closeness_top_3 = cal_node_closeness(G)\n",
    "    degree_top_3 = cal_node_degree(G)\n",
    "    \n",
    "    add_centrality_to_dataframe(central_database, graph_name, betweenness_top_3, 'Betweenness', cal_betweenness_average(G))\n",
    "    add_centrality_to_dataframe(central_database, graph_name, closeness_top_3, 'Closeness', cal_closeness_average(G))\n",
    "    add_centrality_to_dataframe(central_database, graph_name, degree_top_3, 'Degree', cal_degree_average(G))\n",
    "\n",
    "# Save the result to excel file\n",
    "# ❗WARNING❗ It will automatically overwrite the existing file!!!\n",
    "central_database.to_excel(\"result/Database_central.xlsx\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
